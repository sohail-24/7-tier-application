### üîñ Project Context

* **Cluster type**: Self-managed Kubernetes using **kubeadm on AWS EC2**
* **OS**: Ubuntu
* **Nodes**:

  * 1 Control Plane
  * 1 Worker Node
* **CNI**: Calico
* **Philosophy**: GitOps-based architecture (ArgoCD + Helm planned)
* **Database**: ‚ùå NOT inside Kubernetes (separate DB by design)

---

## ‚úÖ COMPLETED STEPS (Verified & Working)

### 1Ô∏è‚É£ kubeadm Cluster Setup

* kubeadm init completed successfully
* kubeconfig configured for non-root user
* Worker node joined successfully
* Both nodes are **Ready**

```bash
kubectl get nodes
```

‚úî Control Plane: Ready
‚úî Worker Node: Ready

---

### 2Ô∏è‚É£ CNI (Calico)

* Calico installed
* Pod IPs assigned (`192.168.x.x`)
* Pod-to-pod communication works

```bash
kubectl get pods -n kube-system
```

---

### 3Ô∏è‚É£ Workload Validation

* nginx deployed
* scaled replicas
* pods running correctly

```bash
kubectl create deployment nginx --image=nginx
kubectl scale deployment nginx --replicas=2
kubectl get pods
```

---

### 4Ô∏è‚É£ Metrics Server (‚ö†Ô∏è HARD PART ‚Äî FIXED)

**Problems faced & fixed:**

* TLS errors
* FailedDiscoveryCheck
* MissingEndpoints
* Port mismatch (10250 vs 4443)
* Service targetPort mismatch
* APIService trust issues
* AWS Security Group blocking node-to-node traffic

**Final Working State:**

* metrics-server Pod: `Running`
* APIService: `True`
* HPA metrics available

```bash
kubectl get apiservices | grep metrics
kubectl top nodes
kubectl top pods
```

‚úî Metrics API = **True**

**Important configs (final):**

* metrics-server listens on **4443**
* Service port `443 ‚Üí targetPort 4443`
* kubelet accessed on **10250**
* SG allows **All traffic from same SG (node ‚Üî node)**

---

### 5Ô∏è‚É£ HPA (Horizontal Pod Autoscaler)

* Resource requests & limits set
* HPA created successfully

```bash
kubectl set resources deployment nginx \
  --requests=cpu=100m --limits=cpu=200m

kubectl autoscale deployment nginx \
  --cpu-percent=50 --min=1 --max=5

kubectl get hpa
```

‚úî HPA object exists
(Load testing pending)

---

### 6Ô∏è‚É£ PodDisruptionBudget (PDB)

* PDB created to ensure availability during disruptions

```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: nginx-pdb
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: nginx
```

```bash
kubectl get pdb
```

‚úî PDB working

---

### 7Ô∏è‚É£ ClusterIP Service (Internal)

* nginx exposed internally
* Verified via busybox pod

```bash
kubectl expose deployment nginx \
  --name=nginx-clusterip \
  --port=80 \
  --target-port=80 \
  --type=ClusterIP
```

```bash
wget http://nginx-clusterip.default.svc.cluster.local
```

‚úî Internal service works

---

### 8Ô∏è‚É£ NetworkPolicies (Zero Trust)

* Default deny policy applied
* Allow traffic ONLY from ingress-nginx namespace

```yaml
# default deny
policyTypes:
- Ingress
```

```yaml
# allow from ingress
namespaceSelector:
  matchLabels:
    kubernetes.io/metadata.name: ingress-nginx
```

```bash
kubectl get networkpolicy
```

‚úî Network isolation enforced correctly

---

### 9Ô∏è‚É£ Ingress Controller (NGINX)

* ingress-nginx deployed using bare-metal manifest
* Controller pod running
* IngressClass created

```bash
kubectl get pods -n ingress-nginx
kubectl get svc -n ingress-nginx
```

---

### üîü Load Balancer (NodePort-based)

* ingress-nginx exposed via NodePort
* Browser access works

Example:

```
http://<EC2_PUBLIC_IP>:30368
```

‚úî Browser shows **Welcome to nginx**

‚ö†Ô∏è Important:

* `http://IP` alone does NOT work
* NodePort must be used
* This is correct for kubeadm (no managed LB)

---

## üìä CURRENT COMPLETION STATUS

| Step                   | Status          |
| ---------------------- | --------------- |
| 1 ‚Üí 10                 | ‚úÖ 100% COMPLETE |
| Frontend (React)       | ‚è≥ Pending       |
| Backend (Node)         | ‚è≥ Pending       |
| GitOps / ArgoCD / Helm | ‚è≥ Next phase    |

---

## üß† KEY ARCHITECTURAL DECISIONS (DO NOT CHANGE)

* ‚ùå No database inside Kubernetes
* ‚úÖ Database will be **separate** (RDS or separate EC2)
* ‚ùå No Skaffold (conflicts with GitOps)
* ‚úÖ GitOps = Git ‚Üí CI ‚Üí Image ‚Üí Helm ‚Üí ArgoCD ‚Üí K8s
* Kubernetes only **runs images**, never builds them

---

## üöÄ WHAT TO DO TOMORROW (NEXT STEPS)

### üîπ Day Next ‚Äî Application + GitOps Phase

#### 1Ô∏è‚É£ Node Backend

* Simple Express API
* Dockerfile
* Helm chart (Deployment + Service)
* No DB inside cluster (use env vars)

#### 2Ô∏è‚É£ React Frontend

* Modify `src/App.js` (real UI)
* Dockerfile
* Helm chart
* Expose via same Ingress

#### 3Ô∏è‚É£ Helm Charts

* `frontend/`
* `backend/`
* `values.yaml` controls image tag

#### 4Ô∏è‚É£ GitOps Setup

* Install ArgoCD
* Connect Git repo
* Auto-sync enabled

---

## üó£Ô∏è HOW TO CONTINUE TOMORROW (IMPORTANT)

When you come back, just say **ONE line**:

> **‚ÄúMentor, continue from React + Node + Helm (cluster already ready)‚Äù**

No need to explain anything again.
I will continue **directly from application + GitOps layer**.

---

### ‚úÖ End of Recovery Notes
